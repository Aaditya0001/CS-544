{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81fc847a-c556-4569-bdc6-2d217b45c233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Live datanodes (2):\n"
     ]
    }
   ],
   "source": [
    "#Q1\n",
    "! hdfs dfsadmin -fs hdfs://boss:9000 -report | grep 'Live datanodes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43cad165-662e-44cf-99aa-b4b01615ea2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-25 17:58:19--  https://pages.cs.wisc.edu/~harter/cs544/data/hdma-wi-2021.csv\n",
      "Resolving pages.cs.wisc.edu (pages.cs.wisc.edu)... 128.105.7.9\n",
      "Connecting to pages.cs.wisc.edu (pages.cs.wisc.edu)|128.105.7.9|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 174944099 (167M) [text/csv]\n",
      "Saving to: ‘/nb/hdma-wi-2021.csv’\n",
      "\n",
      "/nb/hdma-wi-2021.cs 100%[===================>] 166.84M   106MB/s    in 1.6s    \n",
      "\n",
      "2024-10-25 17:58:21 (106 MB/s) - ‘/nb/hdma-wi-2021.csv’ saved [174944099/174944099]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyarrow.csv as pv\n",
    "import pyarrow.parquet as pq\n",
    "csv_path = \"/nb/hdma-wi-2021.csv\"\n",
    "parquet_path = \"/nb/hdma-wi-2021.parquet\"\n",
    "! rm -rf \"/nb/hdma-wi-2021.csv\"\n",
    "! wget -O \"/nb/hdma-wi-2021.csv\" \"https://pages.cs.wisc.edu/~harter/cs544/data/hdma-wi-2021.csv\"\n",
    "\n",
    "# Read CSV file and convert to Parquet\n",
    "table = pv.read_csv(csv_path)\n",
    "pq.write_table(table, parquet_path)\n",
    "\n",
    "# Remove existing Parquet files on HDFS\n",
    "! hdfs dfs -rm -f hdfs://boss:9000/single.parquet\n",
    "! hdfs dfs -rm -f hdfs://boss:9000/double.parquet\n",
    "\n",
    "# Copy Parquet files to HDFS with specified block size and replication\n",
    "! hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=1 -copyFromLocal {parquet_path} hdfs://boss:9000/single.parquet\n",
    "! hdfs dfs -D dfs.block.size=1048576 -D dfs.replication=2 -copyFromLocal {parquet_path} hdfs://boss:9000/double.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7804a815-d352-4708-b750-48ba24aa5950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 M  15.9 M  hdfs://boss:9000/single.parquet\n",
      "15.9 M  31.7 M  hdfs://boss:9000/double.parquet\n"
     ]
    }
   ],
   "source": [
    "#Q2\n",
    "! hdfs dfs -du -h hdfs://boss:9000/single.parquet\n",
    "! hdfs dfs -du -h hdfs://boss:9000/double.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c795f2d3-bcc2-4343-bf1e-bd1e4a92ffbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'FileStatus': {'accessTime': 1729879115355,\n",
       "  'blockSize': 1048576,\n",
       "  'childrenNum': 0,\n",
       "  'fileId': 16386,\n",
       "  'group': 'supergroup',\n",
       "  'length': 16642976,\n",
       "  'modificationTime': 1729879116622,\n",
       "  'owner': 'root',\n",
       "  'pathSuffix': '',\n",
       "  'permission': '644',\n",
       "  'replication': 1,\n",
       "  'storagePolicy': 0,\n",
       "  'type': 'FILE'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q3\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# HDFS URL for the specific file\n",
    "hdfs_url = \"http://boss:9870/webhdfs/v1/single.parquet\"\n",
    "\n",
    "# Parameters for the GETFILESTATUS operation\n",
    "params = {\n",
    "    \"op\": \"GETFILESTATUS\"\n",
    "}\n",
    "\n",
    "# Send GET request to retrieve file status\n",
    "response = requests.get(hdfs_url, params=params)\n",
    "\n",
    "# Raise an error if the request was unsuccessful\n",
    "response.raise_for_status()\n",
    "\n",
    "# Parse the JSON response\n",
    "file_status = response.json()\n",
    "file_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "244ae0d5-a51a-419b-bae7-ab201a0d1c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://93f108f7b020:9864/webhdfs/v1/single.parquet?op=OPEN&namenoderpcaddress=boss:9000&offset=0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q4\n",
    "import requests\n",
    "\n",
    "# HDFS URL for the specific file\n",
    "hdfs_url = \"http://boss:9870/webhdfs/v1/single.parquet\"\n",
    "\n",
    "# Parameters for the OPEN operation\n",
    "params = {\n",
    "    \"op\": \"OPEN\",\n",
    "    \"offset\": 0,\n",
    "    \"noredirect\": \"true\"\n",
    "}\n",
    "\n",
    "# Send GET request to retrieve the block location\n",
    "response = requests.get(hdfs_url, params=params)\n",
    "location = response.json()\n",
    "\n",
    "location['Location']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "548443a7-bce9-4ede-a335-03fed000d3b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'93f108f7b020': 9, '4cfc96f7277b': 7}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q5\n",
    "import requests\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_block_locations(file_path, total_length, block_size):\n",
    "    webhdfs_url = f\"http://boss:9870/webhdfs/v1{file_path}\"\n",
    "    block_distribution = defaultdict(int)\n",
    "\n",
    "    # Loop through each block offset\n",
    "    for offset in range(0, total_length, block_size):\n",
    "        params = {\n",
    "            \"op\": \"OPEN\",\n",
    "            \"offset\": offset,\n",
    "            \"noredirect\": \"true\"\n",
    "        }\n",
    "        \n",
    "        response = requests.get(webhdfs_url, params=params)  \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            location = response_data['Location']\n",
    "            \n",
    "            # Extract container ID from the URL using regex\n",
    "            container_id = re.search(r'http://([^:]+):', location).group(1)\n",
    "            block_distribution[container_id] += 1\n",
    "\n",
    "    return dict(block_distribution)\n",
    "\n",
    "# Fetch file status to get length and block size\n",
    "status_params = {\"op\": \"GETFILESTATUS\"}\n",
    "response = requests.get(\"http://boss:9870/webhdfs/v1/single.parquet\", params=status_params)\n",
    "\n",
    "file_status = response.json()['FileStatus']\n",
    "total_length = file_status['length']\n",
    "block_size = file_status['blockSize']\n",
    "\n",
    "# Get block locations and distribution\n",
    "block_distribution = get_block_locations(file_path=\"/single.parquet\", total_length=total_length, block_size=block_size)\n",
    "block_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a155be4f-ffcd-4c82-a404-c2f8416491cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'93f108f7b020': 16, '4cfc96f7277b': 16}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q6\n",
    "import requests\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_block_distribution(file_path):\n",
    "    webhdfs_url = f\"http://boss:9870/webhdfs/v1{file_path}\"\n",
    "    block_distribution = defaultdict(int)\n",
    "\n",
    "    # Request to get block locations\n",
    "    params = {\n",
    "        \"op\": \"GETFILEBLOCKLOCATIONS\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(webhdfs_url, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        block_locations = response.json()['BlockLocations']['BlockLocation']\n",
    "        \n",
    "        # Iterate through each block location and count the occurrences of each DataNode\n",
    "        for block in block_locations:\n",
    "            for host in block['hosts']:\n",
    "                block_distribution[host] += 1\n",
    "    else:\n",
    "        print(f\"Error fetching block locations: {response.status_code} - {response.json()}\")\n",
    "    \n",
    "    return dict(block_distribution)\n",
    "\n",
    "# Get distribution of blocks for double.parquet\n",
    "block_distribution = get_block_distribution(\"/double.parquet\")\n",
    "block_distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "339c2129-cd65-4119-85b6-10edfe0844bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 17:59:17,381 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "204961.21752386744"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q7\n",
    "import pyarrow as pa\n",
    "import pyarrow.fs\n",
    "import pyarrow.parquet as pq\n",
    "import time\n",
    "import numpy as np\n",
    "import pyarrow.compute as pc\n",
    "\n",
    "\n",
    "# Set HDFS host and port\n",
    "hdfs_host = \"boss\"  # replace with your HDFS host\n",
    "hdfs_port = 9000     # default HDFS port\n",
    "\n",
    "# Create a HadoopFileSystem instance\n",
    "hdfs = pa.fs.HadoopFileSystem(hdfs_host, hdfs_port)\n",
    "\n",
    "# Define the path to the Parquet file in HDFS\n",
    "parquet_file_path = \"/double.parquet\" \n",
    "\n",
    "# Measure the time taken for reading the file and calculating the mean\n",
    "start_time = time.time()\n",
    "\n",
    "# Open the Parquet file\n",
    "with hdfs.open_input_file(parquet_file_path) as f:\n",
    "    table = pq.read_table(f)\n",
    "\n",
    "# Calculate the average loan_amount\n",
    "average_loan_amount = pc.mean(table[\"loan_amount\"]).as_py()\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "# Print results\n",
    "average_loan_amount\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90468d8a-f276-4dda-bb61-3a0cb2e425f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.16151668852819"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q8\n",
    "# Measure the time taken for reading only the loan_amount column\n",
    "start_time_q8 = time.time()\n",
    "\n",
    "# Open the Parquet file and read only the loan_amount column\n",
    "with hdfs.open_input_file(parquet_file_path) as parquet_file:\n",
    "    table = pq.read_table(parquet_file, columns=[\"loan_amount\"])\n",
    "\n",
    "# Calculate the average loan_amount\n",
    "avg_loan_amount_q8 = pc.mean(table[\"loan_amount\"]).as_py()\n",
    "\n",
    "end_time_q8 = time.time()\n",
    "q8_duration = end_time_q8 - start_time_q8\n",
    "\n",
    "# Calculate and print the speedup ratio\n",
    "performance_improvement = elapsed_time / q8_duration\n",
    "performance_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962535e7-4b30-448c-98f2-53963ad05048",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
